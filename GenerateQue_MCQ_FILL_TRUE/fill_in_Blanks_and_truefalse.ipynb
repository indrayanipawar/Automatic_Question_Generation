{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"Final Demo.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"2MMaiVBS4KB3"},"source":["Demo"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"muj6G6lw4RVk","executionInfo":{"status":"ok","timestamp":1619063066915,"user_tz":-330,"elapsed":29059,"user":{"displayName":"Indrayani Pawar","photoUrl":"","userId":"04644843853803718593"}},"outputId":"7d1909cc-a126-4657-ba4a-33995f12571c"},"source":["from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PgLDa-Ji4a3M","executionInfo":{"status":"ok","timestamp":1619063068638,"user_tz":-330,"elapsed":30774,"user":{"displayName":"Indrayani Pawar","photoUrl":"","userId":"04644843853803718593"}},"outputId":"895c55b5-dc9e-4725-eae8-93a1ae209b59"},"source":["%cd drive/MyDrive/task2/Question-Generator/\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/task2/Question-Generator\n"," Data\t\t\t  input1.txt\t  'Question Generation'\n","'Final Demo.ipynb'\t  input_file.txt\n"," generatedQuestions.txt   out_text.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"jFidngus4KB7"},"source":["import nltk\n","import warnings\n","warnings.filterwarnings('ignore')\n","import spacy\n","from spacy import displacy\n","import pandas as pd\n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"75zQdTyf4KB9"},"source":[" def startgeneration(input_file):\n","    print ('\\nChoose from the following : \\n1. Fill in the Blanks \\n2. Multiple Choice Questions \\n3. True or False Questions')\n","\n","    file = open (input_file , 'r')\n","    lines = file.readlines()\n","    text = ''\n","\n","    for i in lines:\n","        text = text + i\n","\n","    text = ' '.join(text.split(\"\\n\"))\n","    print(len(text))\n","    option = int (input())\n","    \n","    flag = 0\n","    \n","    if (option == 1):\n","        print ('\\nFill in the Blanks Question Generation Starting...')\n","        flag = 1\n","    elif (option == 2):\n","        print ('\\nMultiple Choice Question Generation Starting...')   \n","        flag = 1\n","    elif (option == 3):\n","        print ('\\nTrue or False Question Generation Starting...')\n","        flag = 1\n","    else:\n","        print ('\\nInvalid Input')\n","        flag = 0\n","    \n","    if (flag == 1):\n","        generateQuestions(text, option)\n","        print ('Questions Generated and saved in : generatedQuestions.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lk0H07Zx4KB-"},"source":["Pickling"]},{"cell_type":"code","metadata":{"id":"eiOwhOj14KB_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619063101523,"user_tz":-330,"elapsed":8777,"user":{"displayName":"Indrayani Pawar","photoUrl":"","userId":"04644843853803718593"}},"outputId":"1869e670-8ef3-4c1a-f95f-9d4d859388d9"},"source":["!pip install pickle5\n","import pickle5 as cPickle\n","from pathlib import Path\n","\n","def dumpPickle(fileName, content):\n","    pickleFile = open(fileName, 'wb')\n","    cPickle.dump(content, pickleFile, -1)\n","    pickleFile.close()\n","\n","def loadPickle(fileName):    \n","    file = open(fileName, 'rb')\n","    content = cPickle.load(file)\n","    file.close()\n","    \n","    return content\n","    \n","def pickleExists(fileName):\n","    file = Path(fileName)\n","    \n","    if file.is_file():\n","        return True\n","    \n","    return False"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pickle5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/4c/5c4dd0462c8d3a6bc4af500a6af240763c2ebd1efdc736fc2c946d44b70a/pickle5-0.0.11.tar.gz (132kB)\n","\r\u001b[K     |██▌                             | 10kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 12.2MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 30kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 51kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 61kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 81kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 92kB 5.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 102kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 112kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 122kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 5.6MB/s \n","\u001b[?25hBuilding wheels for collected packages: pickle5\n","  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pickle5: filename=pickle5-0.0.11-cp37-cp37m-linux_x86_64.whl size=219268 sha256=486b403469f49ed28d8c32ab66a5ace6070a75a599a67cabf66baf58055f2413\n","  Stored in directory: /root/.cache/pip/wheels/a6/90/95/f889ca4aa8b0e0c7f21c8470b6f5d6032f0390a3a141a9a3bd\n","Successfully built pickle5\n","Installing collected packages: pickle5\n","Successfully installed pickle5-0.0.11\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jeYedZmW4KCC"},"source":["Extract all words from plain text and generate it's features"]},{"cell_type":"code","metadata":{"id":"h4nu8_xV4KCC"},"source":["nlp = spacy.load('en_core_web_sm')\n","\n","#Extract answers and the sentence they are in\n","def extractAnswers(qas, doc):\n","    answers = []\n","\n","    senStart = 0\n","    senId = 0\n","\n","    for sentence in doc.sents:\n","        senLen = len(sentence.text)\n","\n","        for answer in qas:\n","            answerStart = answer['answers'][0]['answer_start']\n","\n","            if (answerStart >= senStart and answerStart < (senStart + senLen)):\n","                answers.append({'sentenceId': senId, 'text': answer['answers'][0]['text']})\n","\n","        senStart += senLen\n","        senId += 1\n","    \n","    return answers\n","\n","#TODO - Clean answers from stopwords?\n","def tokenIsAnswer(token, sentenceId, answers):\n","    for i in range(len(answers)):\n","        if (answers[i]['sentenceId'] == sentenceId):\n","            if (answers[i]['text'] == token):\n","                return True\n","    return False\n","\n","#Save named entities start points\n","\n","def getNEStartIndexs(doc):\n","    neStarts = {}\n","    for ne in doc.ents:\n","        neStarts[ne.start] = ne\n","        \n","    return neStarts \n","\n","def getSentenceStartIndexes(doc):\n","    senStarts = []\n","    \n","    for sentence in doc.sents:\n","        senStarts.append(sentence[0].i)\n","    \n","    return senStarts\n","    \n","def getSentenceForWordPosition(wordPos, senStarts):\n","    for i in range(1, len(senStarts)):\n","        if (wordPos < senStarts[i]):\n","            return i - 1\n","        \n","def addWordsForParagrapgh(newWords, text):\n","    doc = nlp(text)\n","\n","    neStarts = getNEStartIndexs(doc)\n","    senStarts = getSentenceStartIndexes(doc)\n","    \n","    #index of word in spacy doc text\n","    i = 0\n","    \n","    while (i < len(doc)):\n","        #If the token is a start of a Named Entity, add it and push to index to end of the NE\n","        if (i in neStarts):\n","            word = neStarts[i]\n","            #add word\n","            currentSentence = getSentenceForWordPosition(word.start, senStarts)\n","            wordLen = word.end - word.start\n","            shape = ''\n","            for wordIndex in range(word.start, word.end):\n","                shape += (' ' + doc[wordIndex].shape_)\n","\n","            newWords.append([word.text,\n","                            0,\n","                            0,\n","                            currentSentence,\n","                            wordLen,\n","                            word.label_,\n","                            None,\n","                            None,\n","                            None,\n","                            shape])\n","            i = neStarts[i].end - 1\n","        #If not a NE, add the word if it's not a stopword or a non-alpha (not regular letters)\n","        else:\n","            if (doc[i].is_stop == False and doc[i].is_alpha == True):\n","                word = doc[i]\n","\n","                currentSentence = getSentenceForWordPosition(i, senStarts)\n","                wordLen = 1\n","\n","                newWords.append([word.text,\n","                                0,\n","                                0,\n","                                currentSentence,\n","                                wordLen,\n","                                None,\n","                                word.pos_,\n","                                word.tag_,\n","                                word.dep_,\n","                                word.shape_])\n","        i += 1\n","\n","def oneHotEncodeColumns(df):\n","    columnsToEncode = ['NER', 'POS', \"TAG\", 'DEP']\n","\n","    for column in columnsToEncode:\n","        one_hot = pd.get_dummies(df[column])\n","        one_hot = one_hot.add_prefix(column + '_')\n","\n","        df = df.drop(column, axis = 1)\n","        df = df.join(one_hot)\n","    \n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QLCO7X_-4KCE"},"source":["Predict whether a word is a keyword"]},{"cell_type":"code","metadata":{"id":"VIEPJ0tY4KCF"},"source":["def generateDf(text):\n","    words = []\n","    addWordsForParagrapgh(words, text)\n","\n","    wordColums = ['text', 'titleId', 'paragrapghId', 'sentenceId','wordCount', 'NER', 'POS', 'TAG', 'DEP','shape']\n","    df = pd.DataFrame(words, columns=wordColums)\n","    \n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3iPqqebd4KCF"},"source":["def prepareDf(df):\n","    #One-hot encoding\n","    wordsDf = oneHotEncodeColumns(df)\n","\n","    #Drop unused columns\n","    columnsToDrop = ['text', 'titleId', 'paragrapghId', 'sentenceId', 'shape']\n","    wordsDf = wordsDf.drop(columnsToDrop, axis = 1)\n","\n","    #Add missing colums \n","    predictorColumns = ['wordCount', 'NER_CARDINAL', 'NER_DATE', 'NER_EVENT', 'NER_FAC',\n","       'NER_GPE', 'NER_LANGUAGE', 'NER_LAW', 'NER_LOC', 'NER_MONEY',\n","       'NER_NORP', 'NER_ORDINAL', 'NER_ORG', 'NER_PERCENT', 'NER_PERSON',\n","       'NER_PRODUCT', 'NER_QUANTITY', 'NER_TIME', 'NER_WORK_OF_ART', 'POS_ADJ',\n","       'POS_ADP', 'POS_ADV', 'POS_CCONJ', 'POS_DET', 'POS_NOUN', 'POS_NUM',\n","       'POS_PROPN', 'POS_SCONJ', 'POS_VERB', 'TAG_CC', 'TAG_CD', 'TAG_IN',\n","       'TAG_JJ', 'TAG_JJR', 'TAG_JJS', 'TAG_NN', 'TAG_NNP', 'TAG_NNPS',\n","       'TAG_NNS', 'TAG_PDT', 'TAG_RB', 'TAG_RBR', 'TAG_RBS', 'TAG_VB',\n","       'TAG_VBD', 'TAG_VBG', 'TAG_VBN', 'TAG_VBP', 'TAG_VBZ', 'DEP_ROOT',\n","       'DEP_acl', 'DEP_acomp', 'DEP_advcl', 'DEP_advmod', 'DEP_agent',\n","       'DEP_amod', 'DEP_appos', 'DEP_attr', 'DEP_aux', 'DEP_auxpass',\n","       'DEP_ccomp', 'DEP_compound', 'DEP_conj', 'DEP_csubj', 'DEP_csubjpass',\n","       'DEP_dative', 'DEP_dep', 'DEP_dobj', 'DEP_nmod', 'DEP_npadvmod',\n","       'DEP_nsubj', 'DEP_nsubjpass', 'DEP_nummod', 'DEP_oprd', 'DEP_pcomp',\n","       'DEP_pobj', 'DEP_poss', 'DEP_predet', 'DEP_prep', 'DEP_relcl',\n","       'DEP_xcomp']\n","    for feature in predictorColumns:\n","        if feature not in wordsDf.columns:\n","            wordsDf[feature] = 0\n","    return wordsDf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Be16e4qu4KCG"},"source":["def predictWords(wordsDf, df):\n","    \n","    predictorPickleName = 'Data/nb-predictor.pkl'\n","    predictor = loadPickle(predictorPickleName)\n","    \n","    y_pred = predictor.predict_proba(wordsDf)\n","\n","    labeledAnswers = []\n","    for i in range(len(y_pred)):\n","        labeledAnswers.append({'word': df.iloc[i]['text'], 'prob': y_pred[i][0]})\n","    \n","    return labeledAnswers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2GbPh3b_4KCH"},"source":["Extract questions"]},{"cell_type":"code","metadata":{"id":"alW0o0Yi4KCH"},"source":["def blankAnswer(firstTokenIndex, lastTokenIndex, sentStart, sentEnd, doc):\n","    leftPartStart = doc[sentStart].idx\n","    leftPartEnd = doc[firstTokenIndex].idx\n","    rightPartStart = doc[lastTokenIndex].idx + len(doc[lastTokenIndex])\n","    rightPartEnd = doc[sentEnd - 1].idx + len(doc[sentEnd - 1])\n","    \n","    question = doc.text[leftPartStart:leftPartEnd] + '_____' + doc.text[rightPartStart:rightPartEnd]\n","    \n","    return question"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ym4IC3Zf4KCI"},"source":["def addQuestions(answers, text):\n","    doc = nlp(text)\n","    currAnswerIndex = 0\n","    qaPair = []\n","\n","    #Check wheter each token is the next answer\n","    for sent in doc.sents:\n","        for token in sent:\n","            \n","            #If all the answers have been found, stop looking\n","            if currAnswerIndex >= len(answers):\n","                break\n","            \n","            #In the case where the answer is consisted of more than one token, check the following tokens as well.\n","            answerDoc = nlp(answers[currAnswerIndex]['word'])\n","            answerIsFound = True\n","            \n","            for j in range(len(answerDoc)):\n","                if token.i + j >= len(doc) or doc[token.i + j].text != answerDoc[j].text:\n","                    answerIsFound = False\n","           \n","            #If the current token is corresponding with the answer, add it \n","            if answerIsFound:\n","                question = blankAnswer(token.i, token.i + len(answerDoc) - 1, sent.start, sent.end, doc)\n","                \n","                qaPair.append({'question' : question, 'answer': answers[currAnswerIndex]['word'], 'prob': answers[currAnswerIndex]['prob']})\n","                \n","                currAnswerIndex += 1\n","                \n","    return qaPair"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y_H8QzXa4KCJ"},"source":["def sortAnswers(qaPairs):\n","    orderedQaPairs = sorted(qaPairs, key=lambda qaPair: qaPair['prob'])\n","    \n","    return orderedQaPairs    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WQTxJ52j4KCJ"},"source":["Distractors (Incorrect Answers)"]},{"cell_type":"code","metadata":{"id":"siLVazvv4KCJ"},"source":["path = 'Data/distractor.pkl'\n","model = loadPickle(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_84q0vV24KCK"},"source":["def generate_distractors(answer, count):\n","    answer = str.lower(answer)\n","    \n","    ##Extracting closest words for the answer. \n","    try:\n","        closestWords = model.most_similar(positive=[answer], topn=count)\n","    except:\n","        return []\n","    \n","    distractors = list(map(lambda x: x[0], closestWords))[0:count]\n","    \n","    return distractors"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KSCpw_gO4KCK"},"source":["def addDistractors(qaPairs, count):\n","    for qaPair in qaPairs:\n","        distractors = generate_distractors(qaPair['answer'], count)\n","        qaPair['distractors'] = distractors\n","    \n","    return qaPairs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CVlAEgRr4KCL"},"source":["Question Generation Function"]},{"cell_type":"code","metadata":{"id":"-ANyXqtJ4KCL"},"source":["def generateQuestions(text, option):\n","    \n","    # Extract words \n","    df = generateDf(text)\n","    wordsDf = prepareDf(df)\n","\n","    # Predict \n","    labeledAnswers = predictWords(wordsDf, df)\n","\n","    \n","    # Transform questions\n","    qaPairs = addQuestions(labeledAnswers, text)\n","    \n","    # Pick the best questions\n","    orderedQaPairs = sortAnswers(qaPairs)\n","    \n","    # Generate distractors\n","    questions = addDistractors(orderedQaPairs[:len(orderedQaPairs)], 3)\n","\n","    # Formating the output into a txt file\n","    path = 'generatedQuestions.txt'\n","    file = open(path, 'w')\n","    \n","    if (option == 1):\n","        for i in range(len(orderedQaPairs)): \n","            temp = 'Question ' + str(i+1) + '. '\n","            file.write(temp)\n","            file.write(questions[i]['question'])\n","            file.write('\\n')\n","            file.write('Answer : ')\n","            file.write(questions[i]['answer'])\n","            file.write('\\n\\n')     \n","    \n","    if (option == 2):\n","        choice = ['a. ', 'b. ', 'c. ', 'd. ']\n","        for i in range(len(orderedQaPairs)):\n","            temp = 'Question ' + str(i+1) + '. ' \n","            file.write(temp)\n","            file.write(questions[i]['question'])\n","            file.write('\\n')\n","            options = []\n","            a = (questions[i]['answer']).lower()\n","            options.append(a)\n","\n","            for distractor in questions[i]['distractors']:\n","                options.append(distractor)\n","                \n","            random.shuffle(options)\n","            for j in range (len(options)):\n","                temp2 = choice[j] + options[j]\n","                file.write(temp2)\n","                file.write('\\n') \n","            temp3 = 'Answer : ' + a\n","            file.write(temp3)\n","            file.write('\\n\\n')\n","            \n","    if (option == 3):\n","        for i in range(len(orderedQaPairs)):\n","            temp = 'Question ' + str(i+1) + '. ' \n","            file.write(temp)\n","            options = []\n","            options.append(questions[i]['answer'])\n","\n","            for distractor in questions[i]['distractors']:\n","                options.append(distractor)\n","\n","            random.shuffle(options)\n","            inputOption = random.choice(options) \n","\n","            file.write(questions[i]['question'].replace('_____', inputOption))\n","            file.write('\\n')\n","            file.write('Answer : ')\n","\n","            if (inputOption == questions[i]['answer']):\n","                file.write ('True')\n","\n","            else:\n","                file.write ('False')\n","            \n","            file.write ('\\n\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DBbmZK7hZCt1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619064003079,"user_tz":-330,"elapsed":8587,"user":{"displayName":"Indrayani Pawar","photoUrl":"","userId":"04644843853803718593"}},"outputId":"1ef56a84-4912-4e9e-96df-84a51cc7e45e"},"source":["input_file = \"input1.txt\"\n","#input paragraph = \"\" #@param {type:\"string\"}\n","print ('\\nSpell check done and changes made and saved in : ', input_file)\n","startgeneration(input_file)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["\n","Spell check done and changes made and saved in :  input1.txt\n","\n","Choose from the following : \n","1. Fill in the Blanks \n","2. Multiple Choice Questions \n","3. True or False Questions\n","130\n","2\n","\n","Multiple Choice Question Generation Starting...\n","Questions Generated and saved in : generatedQuestions.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IJMa7yr1Nsam"},"source":[""],"execution_count":null,"outputs":[]}]}